{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92254bbb-1a94-40a2-a0a5-42323289daf3",
      "metadata": {
        "id": "92254bbb-1a94-40a2-a0a5-42323289daf3"
      },
      "source": [
        "# Extra JACC Topics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9e830b4-7ec0-4a72-be9b-03c8bb94e4c9",
      "metadata": {
        "id": "a9e830b4-7ec0-4a72-be9b-03c8bb94e4c9"
      },
      "source": [
        "## Finer granularity\n",
        "\n",
        "Defaults for `JACC.parallel_for`:\n",
        "- Synchronize\n",
        "- Use default stream\n",
        "- Compute threads, blocks, and shmem_size\n",
        "\n",
        "```julia\n",
        "@kwdef mutable struct LaunchSpec{Backend}\n",
        "    stream = default_stream(Backend)\n",
        "    threads = 0\n",
        "    blocks = 0\n",
        "    shmem_size::Int = -1\n",
        "    sync::Bool = true\n",
        "end\n",
        "\n",
        "launch_spec(; kw...) = LaunchSpec{typeof(default_backend())}(; kw...)\n",
        "\n",
        "```\n",
        "\n",
        "You can change these defaults using `JACC.launch_spec`.\n",
        "\n",
        "Specify one or more of the keywords and call `JACC.parallel_for(spec, N, f, x…)`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N = 500\n",
        "a_d = JACC.ones(N)\n",
        "JACC.parallel_for(JACC.launch_spec(; sync = false, threads = 1000), N, a_device) do i, a\n",
        "    @inbounds a[i] += 5.0\n",
        "end\n",
        "JACC.synchronize() # because it was non-synchronizing\n",
        ";"
      ],
      "metadata": {
        "id": "3-pIPvWzVLCX"
      },
      "id": "3-pIPvWzVLCX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the macro syntax to make this simpler in most cases."
      ],
      "metadata": {
        "id": "x7vCywjuVjZe"
      },
      "id": "x7vCywjuVjZe"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "f(i, a) = @inbounds a[i] += 5.0\n",
        "\n",
        "JACC.@parallel_for range=N sync=false f(a_d)\n",
        "JACC.synchronize()\n",
        ";"
      ],
      "metadata": {
        "id": "gvXvkEs1VsWC"
      },
      "id": "gvXvkEs1VsWC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are more details about this and `parallel_reduce` but we'll skip those here."
      ],
      "metadata": {
        "id": "7E7xmBnNVevH"
      },
      "id": "7E7xmBnNVevH"
    },
    {
      "cell_type": "markdown",
      "id": "94f4e753-b1ed-4e44-aa2e-01bc9b68309f",
      "metadata": {
        "id": "94f4e753-b1ed-4e44-aa2e-01bc9b68309f"
      },
      "source": [
        "## Shared Memory\n",
        "\n",
        "See the [paper](https://ieeexplore.ieee.org/document/10938453).\n",
        "\n",
        "The `JACC.shared` gives easy access to on-chip GPU shared memory. If you access an array many times per thread, you may get significant speedup by adding just one line.\n",
        "\n",
        "<img src=\"https://github.com/JuliaORNL/JACC-applications/blob/main/tutorial/images/GPU-shared-memory.png?raw=1\" alt=\"GPU shared memory\" style=\"width:45%;height:auto;\">\n",
        "\n",
        "```julia\n",
        "function spectral(i, j, image, filter, num_bands)\n",
        "    for b in 1:num_bands\n",
        "        @inbounds image[b, i, j] *= filter[j]\n",
        "    end\n",
        "end\n",
        "\n",
        "function spectral_shared(i, j, image, filter, num_bands)\n",
        "    filter_shared = JACC.shared(filter) # init shared memory\n",
        "    for b in 1:num_bands\n",
        "        @inbounds image[b, i, j] *= filter_shared[j]\n",
        "    end\n",
        "end\n",
        "\n",
        "num_bands = 60\n",
        "num_voxel = 10_240\n",
        "size_voxel = 64*64\n",
        "image = init_image(Float32, num_bands, num_voxel, size_voxel)\n",
        "filter = init_filter(Float32, size_voxel)\n",
        "jimage = JACC.array(image)\n",
        "jfilter = JACC.array(filter)\n",
        "\n",
        "JACC.parallel_for((num_voxel,size_voxel), spectral, jimage, jfilter, num_bands)\n",
        "# (or)\n",
        "JACC.parallel_for((num_voxel,size_voxel), spectral_shared, jimage, jfilter, num_bands)\n",
        "\n",
        "```\n",
        "\n",
        "Things to keep in mind about `Y = JACC.shared(X)`:\n",
        "- Used inside kernel functions\n",
        "- X is a JACC.array\n",
        "- Y is a copy of X stored in on-chip shared memory\n",
        "- X must fit the shared memory capacity\n",
        "\n",
        "<img src=\"https://github.com/JuliaORNL/JACC-applications/blob/main/tutorial/images/JACC-perfresults-shared.png?raw=1\" alt=\"JACC.shared Performance Results\" style=\"width:90%;height:auto;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3779080a-9875-4f00-9404-58233789ef79",
      "metadata": {
        "id": "3779080a-9875-4f00-9404-58233789ef79"
      },
      "source": [
        "## Exploiting multi-device nodes (EXPERIMENTAL)\n",
        "\n",
        "### `JACC.Multi` : Parallelism on multi-device nodes\n",
        "\n",
        "See the [paper](https://doi.ieeecomputersociety.org/10.1109/eScience65000.2025.00036)\n",
        "\n",
        "Submodule `JACC.Multi`\n",
        "- Deploy JACC’s specification to multi-GPU\n",
        "- Same API (just add `Multi`)\n",
        "\n",
        "JACC.Multi.array: `JX = JACC.Multi.array(X)`\n",
        "- X is evenly distributed into the different GPUs\n",
        "- JX is an backend-specific structure for managing a set of sub-arrays\n",
        "\n",
        "JACC.Multi.parallel_for and JACC.Multi.parallel_reduce\n",
        "- Same API, but...\n",
        "- Launches portion of workload on all devices\n",
        "\n",
        "JACC.Multi support API\n",
        "- `ndev()` : number of available devices\n",
        "- `device_id(a::ArrayPart)` (device) : device where array partition is allocated\n",
        "- `copy!(dest::MultiArray, src::MultiArray)`\n",
        "- `part_length(a::MultiArray)` : get length of each array partition\n",
        "- `sync_ghost_elems!(a::MultiArray` : copy ghost elements between array partitions\n",
        "- `ghost_shift(i::Integer, a::ArrayPart)` (device) get index shifted past starting ghost elements\n",
        "\n",
        "```julia\n",
        "# Unidimensional arrays\n",
        "function axpy(i, alpha, x, y)\n",
        "    x[i] += alpha * y[i]\n",
        "end\n",
        "function dot(i, x, y)\n",
        "    return x[i] * y[i]\n",
        "end\n",
        "SIZE = 1_000_000\n",
        "x = round.(rand(Float64, SIZE) * 100)\n",
        "y = round.(rand(Float64, SIZE) * 100)\n",
        "alpha = 2.5\n",
        "dx = JACC.Multi.array(x)\n",
        "dy = JACC.Multi.array(y)\n",
        "JACC.Multi.parallel_for(SIZE, axpy, alpha, dx, dy)\n",
        "res = JACC.Multi.parallel_reduce(SIZE, dot, dx, dy)\n",
        "\n",
        "# Multidimensional arrays\n",
        "function axpy_2d(i, j, alpha, x, y)\n",
        "    x[i, j] += alpha * y[i, j]\n",
        "end\n",
        "function dot_2d(i, j, x, y)\n",
        "    return x[i, j] * y[i, j]\n",
        "end\n",
        "SIZE = 1_000\n",
        "x = round.(rand(Float64, SIZE, SIZE) * 100)\n",
        "y = round.(rand(Float64, SIZE, SIZE) * 100)\n",
        "alpha = 2.5\n",
        "dx = JACC.Multi.array(x)\n",
        "dy = JACC.Multi.array(y)\n",
        "JACC.Multi.parallel_for((SIZE, SIZE), axpy_2d, alpha, dx, dy)\n",
        "res = JACC.Multi.parallel_reduce((SIZE, SIZE), dot_2d, dx, dy)\n",
        "\n",
        "```\n",
        "\n",
        "<img src=\"https://github.com/JuliaORNL/JACC-applications/blob/main/tutorial/images/JACC-perfresults-multi.png?raw=1\" alt=\"JACC.Multi Performance Results\" style=\"width:90%;height:auto;\">\n",
        "\n",
        "- There are additional functions for managing ghost elements between devices.\n",
        "- _More thorough example in tests_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3864cdec-ffa8-4029-9951-797dba1f59c8",
      "metadata": {
        "id": "3864cdec-ffa8-4029-9951-797dba1f59c8"
      },
      "source": [
        "### `JACC.Async` : Concurrency on multi-device nodes\n",
        "\n",
        "Similar API, just add `Async` and a (user-defined) \"queue\" id\n",
        "\n",
        "```julia\n",
        "JACC.Async.parallel_for(1, ...)\n",
        "JACC.Async.parallel_for(2, ...)\n",
        "\n",
        "# parallel_reduce return value is on device\n",
        "res_d = JACC.Async.parallel_reduce(1, ...)\n",
        "res = JACC.to_host(res_d)[]\n",
        "```\n",
        "\n",
        "JACC.Async support API\n",
        "- `ones(id, ...)`, `zeros(id, ...)`, `fill(id, ...)`\n",
        "- `ndev()` : number of available devices\n",
        "- `synchronize(id)` : finish kernels on specified \"queue\"\n",
        "- `synchronize()` : synchronize all devices\n",
        "\n",
        "_See tests for example._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6630388-3bf9-4d5a-b4e4-e52011f6bad1",
      "metadata": {
        "id": "b6630388-3bf9-4d5a-b4e4-e52011f6bad1"
      },
      "source": [
        "# Applications and Ongoing Efforts\n",
        "\n",
        "Examples and applications using JACC\n",
        "- https://github.com/tdehoff/JACC-7-point-stencil\n",
        "- https://github.com/JuliaORNL/JACC-applications\n",
        "- https://github.com/JuliaORNL/MiniVATES.jl\n",
        "- https://github.com/JuliaORNL/GrayScott.jl\n",
        "\n",
        "Ongoing Efforts\n",
        "- Performance benchmarking (closing gaps)\n",
        "- Expanded API (covering more use-cases)\n",
        "- More intuitive kernel launch\n",
        "- JACC.Auto : Autotuning\n",
        "- Have ideas? Send us a message!\n",
        "\n",
        "Other HPC Julia tutorial resources\n",
        "- SC24 tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d25a266e-5a92-47d3-8717-98d11b9e573f",
      "metadata": {
        "id": "d25a266e-5a92-47d3-8717-98d11b9e573f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia 1.12",
      "language": "julia",
      "name": "julia-1.12"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}